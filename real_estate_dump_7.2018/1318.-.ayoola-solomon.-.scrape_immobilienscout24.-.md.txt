# Scrape immobilienscout24.de, and email new properties found

Finding a new apartment in Berlin is exhausting and slow.
There are too many people in need for a nice apartment.
Many real-estate agents here work on a first-come first-serve basis.
Some apartments are only online for a few hours until they have enough responses.
So there is a need for quick reply after a property has come online.
Another problem with the immobilienscout website, is the filters you can use for searches.
I was looking for an apartment with a garden, that allowed pets. Now there is a filter for the garden,
but no filter to check if pets are allowed, making me checkout many unsuitable apartments.
Lastly, immobilienscout has an e-mail service, sending you an email when a new apartment comes online.
In my experience, this service does not send all apartments that are put on the website.
And it is slow, it is not uncommon to receive the email 5 hours after the apartment is uploaded.

So how does this script solve all these problems for me? making me spend as little time possible looking for new apartments?
* A cron job runs the scraper every 15 minutes, based on a few basic search urls on the website.
* Properties that match my custom filters are stored in a MongoDB database.
* If some of the scraped properties did not already exist in the database, an email is send to the recipients.

This script made me be the first to reply to many apartments (probably confusing quite some people how that is possible).
* 15 minutes is the maximum time between upload of apartment and email sent.


# Installation
 - Clone the repository
    * `npm install` or `yarn install`
 - make an environment and install the requirements (see requirements.txt)
 - Provide the start urls in the scrape_immobilienscout24/spiders/ImmobilienscoutSpider.py file.
 These urls are basic searches on immobilienscout24.de, just follow one of the links that is now in there to have an example.
 - Provide the gmail account details in the mailer/settings.py file.
    * Make sure to allow less secure apps on google to use this feature:
    [https://myaccount.google.com/lesssecureapps](https://myaccount.google.com/lesssecureapps)
 - Provide the recipients email addresses in the mailer/settings.py file
 - To set up a cron job, make sure that the shell and path is set. Use the cron.sh bash file to run scrapy.
    * You will need to update `index.js`
    * `npm start` to start getting new emails as they are uploaded

 - If you just want to run the scraper once:
```
sh cron.sh
```


# Comments
The filters are now hardcoded in the spider. If you want to customize this script for your personal needs, you have to change that.

PS: Do not forget to make the `cron.sh` executable otherwise you will have issues with sudo :)
