# 🏠 immo-feed

[![Build Status](https://travis-ci.org/tamarasaurus/immo-feed.svg?branch=master)](https://travis-ci.org/tamarasaurus/immo-feed)

immo-feed scrapes french real estate websites like leboncoin, ouestfrance, bienici etc. for listings and makes the aggregated results available with a simple api and frontend. it's super easy to create new scraper sources and customise the existing ones to suit your search.

![feed](https://user-images.githubusercontent.com/1336344/41823195-53306a0e-77fc-11e8-84d2-4bcf11dbc702.png)

# How it works

immo-feed is made of:

- a list of customisable sources (websites or json) to scrape for results
- a `runner` that starts the scraper and runs every x minutes
- a mongo database to store the results
- a tiny api to access the results
- a frontend app to view the results
- a way to send notifications for new results (only Slack is available for now)

## Requirements

- node.js (>= v8.1.0)
- mongodb (>= v3.4)
- docker (~v18.03), docker-compose (~v1.19.0) (optional)

# Setup

## To start with docker

```bash
docker-compose up --build
```

To view the logs you can run 

```bash
docker-compose logs -f --tail=10
```

## To start manually

- Make sure mongodb is running

```bash
cd scraper && npm install
cd frontend && npm install

# in three separate processes for each command: 
cd scraper && npm start # starts the scraper
cd scraper && npm run serve # starts the api
cd frontend && npm start # serves the frontend
```

## Accessing the results

### Frontend

Visit `http://localhost:8080` to see and manage the results (sort, hide).

### API

Visit `http://localhost:3000` with these endpoints to acess the API: 

- `GET /results` - list all the results (not yet paginated)
- `GET /results/:id` - get a single result
- `POST /results/:id/hide` - hide a result

## Customising the scraper frequency

You can run the scraper in a couple of ways:

```bash
# manually
cd scraper && SCRAPER_FREQUENCY=15 npm run start

# using docker-compose
SCRAPER_FREQUENCY=15 docker-compose run --rm runner
```

The `SCRAPER_FREQUENCY` environment variable is passed to the runner script, which executes the scrapers every x minutes. 

## Environment variables
- `NOTIFY` - Turn notifications on
- `SLACK_WEBHOOK_URL` - Your webhook url for Slack notifications
- `SCRAPER_FREQUENCY` - How often the scrapers should be run (in minutes)

## Customising scraper sources

### The short explanation

In each file inside `./scraper/src/source/` replace the `url` value with your own search URL for that website (with your own parameters like price or location).

For example, inside `./scraper/src/source/leboncoin.ts`, you can do this:

```diff
- public url = 'https://www.leboncoin.fr/ventes_immobilieres/offres/'
+ public url = 'https://www.leboncoin.fr/recherche/?category=9&real_estate_type=2&price=min-400000&square=25-50'

```

To apply the changes:

```bash
# docker
docker-compose up --build && docker-compose restart

# manually 
cd scraper && npm start 

```

If you visit `http://localhost:8080` you can view the results as they are scraped and stored.

### The long explanation

In the `./scraper/src/source/` folder you will find the default scrapers. Each scraper corresponds to a search url that will be requested and parsed. 

Each result can have these properties:

```javascript
export class Result {
    name: string
    price: number
    size: number
    description: string
    link: string
    image: string
    date: number
}

```

So in each scraper we specify a set of selectors where these attribute can be found.

A scraper can parse a HTML or JSON response. For HTML we receive the whole body of a page and parse it using [Cheerio](https://github.com/cheeriojs/cheerio) (jQuery-like selector syntax). Then we can map elements by selector to a Result attribute.

``` javascript
// Use the HTML scraper type
import { HTMLSource } from '../types/source'

export default class Thierry extends HTMLSource {
    // We give the scraper the search URL that contains the results
    public url = 'https://www.thierry-immobilier.fr/vente/appartement--maison'

    // We define the selector for each list item element
    public resultSelector = '.teaser--immobilier'

    // We map each Result attribute to a selector
    public resultAttributes = [
        { 
            type: 'name', 
            selector: '.teaser__title' 
            
            // Each attribute comes with a default formatter but we can define a custom one to extract the data we want from the `.teaser__title` element
            format($: CheerioStatic, photo: CheerioStatic): string {
                return $(photo).attr('data-whatever')
            }
        },
        { type: 'description', selector: '.teaser__body .dot-ellipsis p' },
        { type: 'size', selector: '.teaser__additional-inner span:nth-child(2)' },
        { type: 'price', selector: '.teaser__price b' },
        { type: 'link', selector: '> a' },
        { type: 'photo', selector: '.field-type-image img', }
    ]
}
```

For a JSON response, it's pretty much the same:

```javascript
import { JSONSource } from '../types/source'

export default class Bienici extends JSONSource {
    // We point to a .json instead of a html page
    public url = 'https://www.bienici.com/realEstateAds.json'

    // We use a JSON property selector that is an array containing the results
    public resultSelector = 'realEstateAds'

    // Each selector is just an object property
    public resultAttributes = [
        { type: 'name', selector: 'title' },
        { type: 'description', selector: 'description' },
        { type: 'size', selector: 'surfaceArea' },
        { type: 'price', selector: 'price' },
        {
            type: 'link',
            selector: 'id',
            format: (id: string) => `https://www.bienici.com/annonce/${id}`
        },
        {
            type: 'photo',
            selector: 'photos',

            // If we want to access an array item at a particular index we can use a custom formatter. In `photos` it will return the array of photos that we specified in the `selector` property above. 
            format: (photos: any) => {
                return photos.length > 0 && photos[0].url
            }
        }
    ]
}
```

## Adding a new scraper source

If you want to add a new scraper, it's pretty simple:

1. Decide whether it's a HTML or JSON source

```javascript
import { HTMLSource } from '../types/source'

export default class MySource extends HTMLSource {
    ...
}

```

2. Provide a search URL that displays a list of results

```javascript
import { HTMLSource } from '../types/source'

export default class MySource extends HTMLSource {
    public url = 'https://www.super-cool-immobilier/nantes/results'
}

```

3. Define the selector for each result element

```javascript
import { HTMLSource } from '../types/source'

export default class MySource extends HTMLSource {
    public url = 'https://www.super-cool-immobilier/nantes/results'
    public resultSelector = 'li.result'
}

```

4. Define where the scraper can find each attribute inside the `li.result` element

```javascript
import { HTMLSource } from '../types/source'

export default class MySource extends HTMLSource {
    public url = 'https://www.super-cool-immobilier/nantes/results'
    public resultSelector = 'li.result'

    public resultAttributes = [
        // So we can find the name of the result inside `li.result .item_title`
        { type: 'name', selector: '.item_title' },
        { type: 'description', selector: '.ispro' },
    ...

```

And that's all you need to do !

5. Rebuild and restart the scrapers

```bash
# docker
docker-compose up --build && docker-compose restart

# manually 
cd scraper && npm start 

```

## Testing

Every scraper in the `./scraper/source/` directory is tested automatically (and each time a new one is added) on Travis CI nightly. To run the tests locally you can do:

```bash
# docker
docker-compose run --rm test npm run test

# manually
cd scraper && npm test
```
